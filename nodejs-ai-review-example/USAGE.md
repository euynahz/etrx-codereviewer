# 🎯 AI代码评审调试工具 - 使用指南

## 🚀 快速开始

### Windows用户（推荐）
直接双击 `start.bat` 文件即可！脚本会自动：
- 安装所有Node.js依赖
- 启动Web服务
- 打开浏览器到调试界面

### 手动启动
```bash
npm install  # 安装依赖
npm start    # 启动服务
```
然后访问 http://localhost:3000

## 🎨 Web界面功能详解

### 左侧 - 提示词配置区
1. **AI连接状态** - 显示AI服务连接状态，可点击"测试连接"验证
2. **模板选择器** - 5个预设模板 + 自定义选项：
   - 简洁代码评审（推荐调试用）
   - 详细代码评审  
   - 后端代码评审
   - 前端代码评审
   - 开发手册评审
   - 自定义模板
3. **提示词编辑框** - 支持语法高亮，显示字符统计
4. **示例代码预览** - 显示将要评审的示例代码变更

### 右侧 - 评审结果展示区
1. **加载动画** - 评审进行中的动态效果
2. **结果元信息** - 评审ID、使用模型、耗时等
3. **Markdown渲染** - 完整支持Markdown格式显示
4. **快捷操作** - 一键复制结果或提示词

## 🔧 调试提示词的最佳实践

### 1. 从默认模板开始
- 先选择"简洁代码评审"模板测试基本功能
- 观察AI的响应格式和内容质量

### 2. 逐步调整提示词
- 复制默认模板到"自定义模板"
- 逐一修改指令，观察效果变化
- 重点调整：输出格式、关注重点、语言风格

### 3. 观察关键指标
- **响应时间** - 提示词过长会影响速度
- **输出格式** - 确保AI遵循Markdown格式要求
- **内容质量** - 问题识别准确性和建议实用性

### 4. 常见调试技巧
- 在提示词末尾强调"不要输出代码包裹"
- 使用具体的输出格式示例
- 明确指出要避免的行为（如过度解释）

## 📝 示例调试流程

1. **基础测试**
   ```
   选择"简洁代码评审" → 点击"开始评审" → 观察基础效果
   ```

2. **自定义调试**
   ```
   选择"自定义模板" → 输入修改后的提示词 → 测试效果 → 继续优化
   ```

3. **效果对比**
   ```
   保存当前结果 → 修改提示词 → 重新评审 → 对比差异
   ```

## 🛠️ 配置说明

### AI服务配置（在 index.js 中修改）
```javascript
const reviewer = new AICodeReviewer({
    modelName: "qwen3:8b",           // AI模型名称
    endpoint: "http://192.168.66.181:11434",  // Ollama服务地址
    temperature: 0.7,                // 输出随机性（0-2）
    maxTokens: 2048,                 // 最大输出长度
    timeout: 30000,                  // 请求超时时间
    retryCount: 3                    // 重试次数
});
```

### 示例代码（在 index.js 中修改）
可以修改 `sampleCodeChanges` 数组来测试不同类型的代码变更。

## 🐛 故障排除

### 连接问题
- 确认Ollama服务正在运行
- 检查端点地址是否正确
- 确认模型已下载（qwen3:8b）

### 界面问题
- 刷新页面重试
- 检查浏览器控制台错误信息
- 确认端口3000未被占用

### 评审问题
- 检查提示词是否包含`{code}`占位符
- 尝试减少提示词长度
- 调整timeout配置

## 💡 提示词优化建议

### 1. 格式控制
```
明确要求：
- "以Markdown格式输出"
- "不要使用代码包裹"
- "返回结果过滤思考过程"
```

### 2. 内容控制
```
设置边界：
- "简洁明了，只指出重要问题"
- "每个问题控制在3-4句话内"
- "如果代码质量良好，简单说明即可"
```

### 3. 结构化输出
```
提供模板：
## 📝 评审总结
[具体要求]

## 🔍 发现的问题
[具体要求]

## 💡 优化建议
[具体要求]
```

---

开始你的AI代码评审提示词调试之旅吧！🎉